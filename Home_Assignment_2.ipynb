{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae580f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.api import OLS, add_constant\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "\n",
    "Postures = pd.read_csv(\"Postures.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad08a3",
   "metadata": {},
   "source": [
    "# 1) Processing the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f563055-6902-49bf-8c0d-f1f9cab49677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of missing values in column X3: 0.88%\n",
      "Proportion of missing values in column Y3: 0.88%\n",
      "Proportion of missing values in column Z3: 0.88%\n",
      "Proportion of missing values in column X4: 4.0%\n",
      "Proportion of missing values in column Y4: 4.0%\n",
      "Proportion of missing values in column Z4: 4.0%\n",
      "Proportion of missing values in column X5: 16.68%\n",
      "Proportion of missing values in column Y5: 16.68%\n",
      "Proportion of missing values in column Z5: 16.68%\n",
      "Proportion of missing values in column X6: 33.1%\n",
      "Proportion of missing values in column Y6: 33.1%\n",
      "Proportion of missing values in column Z6: 33.1%\n",
      "Proportion of missing values in column X7: 50.13%\n",
      "Proportion of missing values in column Y7: 50.13%\n",
      "Proportion of missing values in column Z7: 50.13%\n",
      "Proportion of missing values in column X8: 60.86%\n",
      "Proportion of missing values in column Y8: 60.86%\n",
      "Proportion of missing values in column Z8: 60.86%\n",
      "Proportion of missing values in column X9: 69.31%\n",
      "Proportion of missing values in column Y9: 69.31%\n",
      "Proportion of missing values in column Z9: 69.31%\n",
      "Proportion of missing values in column X10: 81.11%\n",
      "Proportion of missing values in column Y10: 81.11%\n",
      "Proportion of missing values in column Z10: 81.11%\n",
      "Proportion of missing values in column X11: 99.96%\n",
      "Proportion of missing values in column Y11: 99.96%\n",
      "Proportion of missing values in column Z11: 99.96%\n"
     ]
    }
   ],
   "source": [
    "# Eliminate first instance of Postures (all 0's) \n",
    "df = Postures.iloc[1:]\n",
    "\n",
    "for col in df.columns:\n",
    "    proportion = (df[col] == '?').mean()*100\n",
    "    if proportion > 0:\n",
    "        print(f'Proportion of missing values in column {col}: {round(proportion,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f51b632e-8132-4b0f-85bf-3525bb2a9025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#removing the variables with a proportion of missing values more than 80% \n",
    "for col in df.columns:\n",
    "    proportion = (df[col] == '?').mean()*100\n",
    "    if proportion > 80:\n",
    "        df=df.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb576599",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace all '?' to NaN, so that the values are valid for Imputation\n",
    "for col in df.columns:\n",
    "    df.loc[df[col] == '?', col] = np.nan\n",
    "    \n",
    "# Consider the User variable as categorical (even though it is numeric)\n",
    "Postures['User'] = Postures['User'].astype('category')\n",
    "\n",
    "# Instatiate a KNN Imputater\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "\n",
    "# Extract from the Data Set the X and Y\n",
    "# WARNING: For testing purposes, only work with a small sub-set of the original Data Set\n",
    "#         Should be replaced for the whole Data Set in the act of Delivery \n",
    "X= df.values[0:10000,1:38]    \n",
    "y=df['Class'].values[0:10000]        \n",
    "        \n",
    "\n",
    "    \n",
    "# Acquire a new DataFrame with Imputated Values \n",
    "Xt=pd.DataFrame(imputer.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1bb377",
   "metadata": {},
   "source": [
    "# 2) Tree Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54e18f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 9, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "The Precision is:  0.9488\n",
      "The Recall is:  0.9488\n",
      "The F1 score is:  0.9488\n",
      "The Matthews correlation coefficient is:  0.9350\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1    2    3    4\n",
      "0  718    2    3    2    1\n",
      "1    3  334    0    0   15\n",
      "2   13    7  409    4   11\n",
      "3    1    1    4  438   15\n",
      "4    5   20   10   11  473\n",
      "Cross-validated Accuracy: 0.9363999999999999\n",
      "Execution time: 7.5226218700408936 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Feature scaling (not strictly necessary for decision trees, but good practice for other algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(Xt)\n",
    "\n",
    "# Divide the whole Set into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=25)\n",
    "\n",
    "# Create a Decision Tree Model with the data\n",
    "tree_mdl = DecisionTreeClassifier()\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV (taking ~53 seconds)\n",
    "parameters = {\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [5, 7, 9],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(tree_mdl, parameters, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Use the best hyperparameters to create the final model\n",
    "final_tree_mdl = DecisionTreeClassifier(**best_params)\n",
    "final_tree_mdl.fit(X_train, y_train)\n",
    "preds = final_tree_mdl.predict(X_test)\n",
    "\n",
    "# Evaluate the results\n",
    "print(\"The Precision is: %7.4f\" % precision_score(y_test, preds, average='micro'))\n",
    "print(\"The Recall is: %7.4f\" % recall_score(y_test, preds, average='micro'))\n",
    "print(\"The F1 score is: %7.4f\" % f1_score(y_test, preds, average='micro'))\n",
    "print(\"The Matthews correlation coefficient is: %7.4f\" % matthews_corrcoef(y_test, preds))\n",
    "print()\n",
    "print(\"This is the Confusion Matrix\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, preds)))\n",
    "\n",
    "# Cross-validated accuracy\n",
    "cv_accuracy = cross_val_score(final_tree_mdl, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-validated Accuracy:\", np.mean(cv_accuracy))\n",
    "\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29e0ba",
   "metadata": {},
   "source": [
    "# 3) Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Regression Model with the data\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Present the Biases\n",
    "print(\"The bias is: \",  reg.intercept_)\n",
    "print(\"The other parameters are: \")\n",
    "for i, beta in enumerate(reg.coef_):\n",
    "    print(\"\\t B%d -> %9.3f\"% (i+1, beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc210c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Results\n",
    "preds=reg.predict(X_test)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(preds, y_test)\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Testing Set')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = add_constant(X_train)\n",
    "mdl=OLS(y_train,X_tr, hasconst=12).fit()\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19852e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do: The same for Lasso, Ridge aand Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead9903",
   "metadata": {},
   "source": [
    "# 4) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data so it can be used in Naive Bayes Models\n",
    "scaler = MinMaxScaler() #Maybe use different Scalers (See info in TP06)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Column Names (a.k.a Possible Classes)\n",
    "classes = np.array(['Class 1','Class 2','Class 3','Class 4','Class 5'])\n",
    "\n",
    "# Create a Gaussian Naive Bayes Model with the scaled data\n",
    "gnb=GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Present the Results\n",
    "preds=gnb.predict(X_test)\n",
    "print(\"The Accuracy score is: \", accuracy_score(y_test, preds))\n",
    "print()\n",
    "print('Confusion Matrix:')\n",
    "display(pd.DataFrame(confusion_matrix(y_test, preds), columns=classes, index=classes))\n",
    "\n",
    "\n",
    "# Create a Categorical Naive Bayes Model with the scaled data\n",
    "cnb=CategoricalNB()\n",
    "cnb.fit(X_train,y_train)\n",
    "\n",
    "# Present the Results\n",
    "preds=cnb.predict(X_test)\n",
    "print(\"The Accuracy score is: \", accuracy_score(y_test, preds))\n",
    "print()\n",
    "print('Confusion Matrix:')\n",
    "display(pd.DataFrame(confusion_matrix(y_test, preds), columns=classes, index=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf82ac",
   "metadata": {},
   "source": [
    "# 5) K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b65b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the whole Set into Training and Testing Sets to be scaled with a different Scaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xt, y, test_size=0.25, random_state=25)\n",
    "\n",
    "# Scale the data so it can be used in K-Nearest Neighbours Models\n",
    "scaler = StandardScaler() #Maybe use different Scalers (See info in TP06)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "preds = knn.predict(X_test)\n",
    "\n",
    "# Present the Results\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#Sholud try to make plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5da97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
