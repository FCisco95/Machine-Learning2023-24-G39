{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef74eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, PowerTransformer\n",
    "\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "Postures = pd.read_csv(\"Postures.csv\")\n",
    "\n",
    "def printClassResults(truth, preds):\n",
    "    print(\"The Accuracy is: %7.4f\" % accuracy_score(truth, preds))\n",
    "    print(\"The Precision is: %7.4f\" % precision_score(y_test, preds,average='micro'))\n",
    "    print(\"The Recall is: %7.4f\"    % recall_score(y_test, preds,average='micro'))\n",
    "    print(\"The F1 score is: %7.4f\"  % f1_score(y_test, preds,average='micro'))\n",
    "    print(\"The Matthews correlation coefficient is: %7.4f\" % matthews_corrcoef(y_test, preds))\n",
    "    print()\n",
    "    print(\"This is the Confusion Matrix\")\n",
    "    display(pd.DataFrame(confusion_matrix(truth, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf5f06a",
   "metadata": {},
   "source": [
    "# 1) Processing the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3efd5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate first instance of Postures (all 0's) \n",
    "df = Postures.iloc[1:]\n",
    "\n",
    "#Removing the variables with a proportion of missing values more than 80% \n",
    "for col in df.columns:\n",
    "    proportion = (df[col] == '?').mean()*100\n",
    "    if proportion > 80:\n",
    "        df=df.drop(col, axis=1)\n",
    "        \n",
    "        \n",
    "# Replace all '?' to NaN, so that the values are valid for Imputation\n",
    "for col in df.columns:\n",
    "    df.loc[df[col] == '?', col] = np.nan\n",
    "    \n",
    "    \n",
    "# Extract from the Data Set the X and Y\n",
    "# WARNING: For testing purposes, only work with a small sub-set of the original Data Set\n",
    "#         Should be replaced for the whole Data Set in the act of Delivery \n",
    "X= df.values[0:10000,1:38]    \n",
    "y= df['Class'].values[0:10000]\n",
    "\n",
    "# Instatiate a KNN Imputater\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "\n",
    "# Acquire a new DataFrame with Imputated Values \n",
    "Xt=pd.DataFrame(imputer.fit_transform(X))\n",
    "    \n",
    "# Divide the whole Set into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xt, y, test_size=0.25, random_state=25)\n",
    "\n",
    "# Scaling the data\n",
    "scaler   = PowerTransformer()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7da0dd",
   "metadata": {},
   "source": [
    "# 3) Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc63fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.594</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.592</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   352.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 19 Nov 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:29:10</td>     <th>  Log-Likelihood:    </th> <td> -10296.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  7500</td>      <th>  AIC:               </th> <td>2.066e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  7468</td>      <th>  BIC:               </th> <td>2.088e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    31</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    2.8700</td> <td>    0.011</td> <td>  259.734</td> <td> 0.000</td> <td>    2.848</td> <td>    2.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.5180</td> <td>    0.014</td> <td>  -37.392</td> <td> 0.000</td> <td>   -0.545</td> <td>   -0.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.1859</td> <td>    0.015</td> <td>  -12.649</td> <td> 0.000</td> <td>   -0.215</td> <td>   -0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.2918</td> <td>    0.015</td> <td>   19.185</td> <td> 0.000</td> <td>    0.262</td> <td>    0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.3486</td> <td>    0.018</td> <td>  -19.588</td> <td> 0.000</td> <td>   -0.383</td> <td>   -0.314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.2216</td> <td>    0.014</td> <td>  -15.559</td> <td> 0.000</td> <td>   -0.250</td> <td>   -0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.3205</td> <td>    0.015</td> <td>   21.450</td> <td> 0.000</td> <td>    0.291</td> <td>    0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.2953</td> <td>    0.018</td> <td>  -16.599</td> <td> 0.000</td> <td>   -0.330</td> <td>   -0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.2767</td> <td>    0.014</td> <td>  -19.183</td> <td> 0.000</td> <td>   -0.305</td> <td>   -0.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.3027</td> <td>    0.015</td> <td>   20.442</td> <td> 0.000</td> <td>    0.274</td> <td>    0.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.2818</td> <td>    0.018</td> <td>  -15.508</td> <td> 0.000</td> <td>   -0.317</td> <td>   -0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -0.2007</td> <td>    0.014</td> <td>  -14.134</td> <td> 0.000</td> <td>   -0.229</td> <td>   -0.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.2504</td> <td>    0.015</td> <td>   16.624</td> <td> 0.000</td> <td>    0.221</td> <td>    0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.1252</td> <td>    0.018</td> <td>   -6.839</td> <td> 0.000</td> <td>   -0.161</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>   -0.1281</td> <td>    0.014</td> <td>   -9.072</td> <td> 0.000</td> <td>   -0.156</td> <td>   -0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>    0.1695</td> <td>    0.015</td> <td>   11.042</td> <td> 0.000</td> <td>    0.139</td> <td>    0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>   -0.0053</td> <td>    0.018</td> <td>   -0.288</td> <td> 0.773</td> <td>   -0.041</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>   -0.0500</td> <td>    0.012</td> <td>   -4.065</td> <td> 0.000</td> <td>   -0.074</td> <td>   -0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>    0.1858</td> <td>    0.013</td> <td>   14.577</td> <td> 0.000</td> <td>    0.161</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.0665</td> <td>    0.015</td> <td>    4.439</td> <td> 0.000</td> <td>    0.037</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -0.0611</td> <td>    0.012</td> <td>   -4.998</td> <td> 0.000</td> <td>   -0.085</td> <td>   -0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>    0.0594</td> <td>    0.012</td> <td>    4.765</td> <td> 0.000</td> <td>    0.035</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.1778</td> <td>    0.015</td> <td>   12.163</td> <td> 0.000</td> <td>    0.149</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>   -0.0459</td> <td>    0.013</td> <td>   -3.559</td> <td> 0.000</td> <td>   -0.071</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.1301</td> <td>    0.013</td> <td>   10.175</td> <td> 0.000</td> <td>    0.105</td> <td>    0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>    0.1415</td> <td>    0.015</td> <td>    9.159</td> <td> 0.000</td> <td>    0.111</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.0205</td> <td>    0.013</td> <td>   -1.627</td> <td> 0.104</td> <td>   -0.045</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>   -0.1279</td> <td>    0.013</td> <td>  -10.124</td> <td> 0.000</td> <td>   -0.153</td> <td>   -0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>    0.1037</td> <td>    0.013</td> <td>    7.780</td> <td> 0.000</td> <td>    0.078</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>    0.0371</td> <td>    0.013</td> <td>    2.912</td> <td> 0.004</td> <td>    0.012</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>   -0.1006</td> <td>    0.013</td> <td>   -7.689</td> <td> 0.000</td> <td>   -0.126</td> <td>   -0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>    0.1040</td> <td>    0.014</td> <td>    7.614</td> <td> 0.000</td> <td>    0.077</td> <td>    0.131</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>402.172</td> <th>  Durbin-Watson:     </th> <td>   2.052</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1243.250</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.228</td>  <th>  Prob(JB):          </th> <td>1.08e-270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.942</td>  <th>  Cond. No.          </th> <td>    4.24</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.594   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.592   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     352.0   \\\\\n",
       "\\textbf{Date:}             & Sun, 19 Nov 2023 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}             &     18:29:10     & \\textbf{  Log-Likelihood:    } &   -10296.   \\\\\n",
       "\\textbf{No. Observations:} &        7500      & \\textbf{  AIC:               } & 2.066e+04   \\\\\n",
       "\\textbf{Df Residuals:}     &        7468      & \\textbf{  BIC:               } & 2.088e+04   \\\\\n",
       "\\textbf{Df Model:}         &          31      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       2.8700  &        0.011     &   259.734  &         0.000        &        2.848    &        2.892     \\\\\n",
       "\\textbf{x1}    &      -0.5180  &        0.014     &   -37.392  &         0.000        &       -0.545    &       -0.491     \\\\\n",
       "\\textbf{x2}    &      -0.1859  &        0.015     &   -12.649  &         0.000        &       -0.215    &       -0.157     \\\\\n",
       "\\textbf{x3}    &       0.2918  &        0.015     &    19.185  &         0.000        &        0.262    &        0.322     \\\\\n",
       "\\textbf{x4}    &      -0.3486  &        0.018     &   -19.588  &         0.000        &       -0.383    &       -0.314     \\\\\n",
       "\\textbf{x5}    &      -0.2216  &        0.014     &   -15.559  &         0.000        &       -0.250    &       -0.194     \\\\\n",
       "\\textbf{x6}    &       0.3205  &        0.015     &    21.450  &         0.000        &        0.291    &        0.350     \\\\\n",
       "\\textbf{x7}    &      -0.2953  &        0.018     &   -16.599  &         0.000        &       -0.330    &       -0.260     \\\\\n",
       "\\textbf{x8}    &      -0.2767  &        0.014     &   -19.183  &         0.000        &       -0.305    &       -0.248     \\\\\n",
       "\\textbf{x9}    &       0.3027  &        0.015     &    20.442  &         0.000        &        0.274    &        0.332     \\\\\n",
       "\\textbf{x10}   &      -0.2818  &        0.018     &   -15.508  &         0.000        &       -0.317    &       -0.246     \\\\\n",
       "\\textbf{x11}   &      -0.2007  &        0.014     &   -14.134  &         0.000        &       -0.229    &       -0.173     \\\\\n",
       "\\textbf{x12}   &       0.2504  &        0.015     &    16.624  &         0.000        &        0.221    &        0.280     \\\\\n",
       "\\textbf{x13}   &      -0.1252  &        0.018     &    -6.839  &         0.000        &       -0.161    &       -0.089     \\\\\n",
       "\\textbf{x14}   &      -0.1281  &        0.014     &    -9.072  &         0.000        &       -0.156    &       -0.100     \\\\\n",
       "\\textbf{x15}   &       0.1695  &        0.015     &    11.042  &         0.000        &        0.139    &        0.200     \\\\\n",
       "\\textbf{x16}   &      -0.0053  &        0.018     &    -0.288  &         0.773        &       -0.041    &        0.031     \\\\\n",
       "\\textbf{x17}   &      -0.0500  &        0.012     &    -4.065  &         0.000        &       -0.074    &       -0.026     \\\\\n",
       "\\textbf{x18}   &       0.1858  &        0.013     &    14.577  &         0.000        &        0.161    &        0.211     \\\\\n",
       "\\textbf{x19}   &       0.0665  &        0.015     &     4.439  &         0.000        &        0.037    &        0.096     \\\\\n",
       "\\textbf{x20}   &      -0.0611  &        0.012     &    -4.998  &         0.000        &       -0.085    &       -0.037     \\\\\n",
       "\\textbf{x21}   &       0.0594  &        0.012     &     4.765  &         0.000        &        0.035    &        0.084     \\\\\n",
       "\\textbf{x22}   &       0.1778  &        0.015     &    12.163  &         0.000        &        0.149    &        0.206     \\\\\n",
       "\\textbf{x23}   &      -0.0459  &        0.013     &    -3.559  &         0.000        &       -0.071    &       -0.021     \\\\\n",
       "\\textbf{x24}   &       0.1301  &        0.013     &    10.175  &         0.000        &        0.105    &        0.155     \\\\\n",
       "\\textbf{x25}   &       0.1415  &        0.015     &     9.159  &         0.000        &        0.111    &        0.172     \\\\\n",
       "\\textbf{x26}   &      -0.0205  &        0.013     &    -1.627  &         0.104        &       -0.045    &        0.004     \\\\\n",
       "\\textbf{x27}   &      -0.1279  &        0.013     &   -10.124  &         0.000        &       -0.153    &       -0.103     \\\\\n",
       "\\textbf{x28}   &       0.1037  &        0.013     &     7.780  &         0.000        &        0.078    &        0.130     \\\\\n",
       "\\textbf{x29}   &       0.0371  &        0.013     &     2.912  &         0.004        &        0.012    &        0.062     \\\\\n",
       "\\textbf{x30}   &      -0.1006  &        0.013     &    -7.689  &         0.000        &       -0.126    &       -0.075     \\\\\n",
       "\\textbf{x31}   &       0.1040  &        0.014     &     7.614  &         0.000        &        0.077    &        0.131     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 402.172 & \\textbf{  Durbin-Watson:     } &     2.052  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &  1243.250  \\\\\n",
       "\\textbf{Skew:}          &   0.228 & \\textbf{  Prob(JB):          } & 1.08e-270  \\\\\n",
       "\\textbf{Kurtosis:}      &   4.942 & \\textbf{  Cond. No.          } &      4.24  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.594\n",
       "Model:                            OLS   Adj. R-squared:                  0.592\n",
       "Method:                 Least Squares   F-statistic:                     352.0\n",
       "Date:                Sun, 19 Nov 2023   Prob (F-statistic):               0.00\n",
       "Time:                        18:29:10   Log-Likelihood:                -10296.\n",
       "No. Observations:                7500   AIC:                         2.066e+04\n",
       "Df Residuals:                    7468   BIC:                         2.088e+04\n",
       "Df Model:                          31                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          2.8700      0.011    259.734      0.000       2.848       2.892\n",
       "x1            -0.5180      0.014    -37.392      0.000      -0.545      -0.491\n",
       "x2            -0.1859      0.015    -12.649      0.000      -0.215      -0.157\n",
       "x3             0.2918      0.015     19.185      0.000       0.262       0.322\n",
       "x4            -0.3486      0.018    -19.588      0.000      -0.383      -0.314\n",
       "x5            -0.2216      0.014    -15.559      0.000      -0.250      -0.194\n",
       "x6             0.3205      0.015     21.450      0.000       0.291       0.350\n",
       "x7            -0.2953      0.018    -16.599      0.000      -0.330      -0.260\n",
       "x8            -0.2767      0.014    -19.183      0.000      -0.305      -0.248\n",
       "x9             0.3027      0.015     20.442      0.000       0.274       0.332\n",
       "x10           -0.2818      0.018    -15.508      0.000      -0.317      -0.246\n",
       "x11           -0.2007      0.014    -14.134      0.000      -0.229      -0.173\n",
       "x12            0.2504      0.015     16.624      0.000       0.221       0.280\n",
       "x13           -0.1252      0.018     -6.839      0.000      -0.161      -0.089\n",
       "x14           -0.1281      0.014     -9.072      0.000      -0.156      -0.100\n",
       "x15            0.1695      0.015     11.042      0.000       0.139       0.200\n",
       "x16           -0.0053      0.018     -0.288      0.773      -0.041       0.031\n",
       "x17           -0.0500      0.012     -4.065      0.000      -0.074      -0.026\n",
       "x18            0.1858      0.013     14.577      0.000       0.161       0.211\n",
       "x19            0.0665      0.015      4.439      0.000       0.037       0.096\n",
       "x20           -0.0611      0.012     -4.998      0.000      -0.085      -0.037\n",
       "x21            0.0594      0.012      4.765      0.000       0.035       0.084\n",
       "x22            0.1778      0.015     12.163      0.000       0.149       0.206\n",
       "x23           -0.0459      0.013     -3.559      0.000      -0.071      -0.021\n",
       "x24            0.1301      0.013     10.175      0.000       0.105       0.155\n",
       "x25            0.1415      0.015      9.159      0.000       0.111       0.172\n",
       "x26           -0.0205      0.013     -1.627      0.104      -0.045       0.004\n",
       "x27           -0.1279      0.013    -10.124      0.000      -0.153      -0.103\n",
       "x28            0.1037      0.013      7.780      0.000       0.078       0.130\n",
       "x29            0.0371      0.013      2.912      0.004       0.012       0.062\n",
       "x30           -0.1006      0.013     -7.689      0.000      -0.126      -0.075\n",
       "x31            0.1040      0.014      7.614      0.000       0.077       0.131\n",
       "==============================================================================\n",
       "Omnibus:                      402.172   Durbin-Watson:                   2.052\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1243.250\n",
       "Skew:                           0.228   Prob(JB):                    1.08e-270\n",
       "Kurtosis:                       4.942   Cond. No.                         4.24\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr = add_constant(X_train)\n",
    "mdl=OLS(y_train,X_tr, hasconst=12).fit()\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7265eb1",
   "metadata": {},
   "source": [
    "## 3.1) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eebba5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver: newton-cg | accuracy_score: 0.8776\n",
      "Solver: lbfgs | accuracy_score: 0.8776\n",
      "Solver: liblinear | accuracy_score: 0.868\n",
      "Solver: sag | accuracy_score: 0.8776\n",
      "Solver: saga | accuracy_score: 0.8776\n",
      "»Best Solver: newton-cg : 0.8776\n",
      "###\n",
      "C: 100 | accuracy_score: 0.8784\n",
      "C: 10 | accuracy_score: 0.878\n",
      "C: 1.0 | accuracy_score: 0.8776\n",
      "C: 0.1 | accuracy_score: 0.8768\n",
      "C: 0.01 | accuracy_score: 0.8704\n",
      "»Best C: 100 : 0.8784\n",
      "###\n",
      "»» Best(?) Model Accuracy: 0.8784 |Solver: newton-cg |C value: 100\n"
     ]
    }
   ],
   "source": [
    "# Create various lists with different Hyperparameter values\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear','sag','saga']\n",
    "penalty = ['l2','l1','elasticnet', None]\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "best_solver = ''\n",
    "best_solver_value = 0\n",
    "\n",
    "best_C = ''\n",
    "best_C_value = 0\n",
    "\n",
    "# Find the best Solver Hyperparameter\n",
    "for solver in solvers:\n",
    "    # Create Logistic Regression Model using a specific Solver\n",
    "    LR = LogisticRegression(max_iter=100000, solver=solver).fit(X_train, y_train)\n",
    "\n",
    "    # Get Accuracy\n",
    "    preds = LR.predict(X_test)\n",
    "    a = accuracy_score(y_test, preds)\n",
    "    if a > best_solver_value:\n",
    "        best_solver_value = a\n",
    "        best_solver = solver\n",
    "    print(\"Solver:\", solver, \"| accuracy_score:\", a)\n",
    "\n",
    "print('»Best Solver:', best_solver, ':', best_solver_value)\n",
    "print('###')  \n",
    "\n",
    "\n",
    "for c in c_values:\n",
    "    # Create Logistic Regression Model using a specific C value\n",
    "    LR = LogisticRegression(max_iter=100000, C=c).fit(X_train, y_train)\n",
    "\n",
    "    # Get Accuracy\n",
    "    preds = LR.predict(X_test)\n",
    "    a = accuracy_score(y_test, preds)\n",
    "    if a > best_C_value:\n",
    "        best_C_value = a\n",
    "        best_C = c\n",
    "    print(\"C:\", c, \"| accuracy_score:\", a)\n",
    "    \n",
    "print('»Best C:', best_C, ':', best_C_value)\n",
    "print('###')\n",
    "\n",
    "# Create a Logistic Regression Model with the best Hyperparameters found\n",
    "LR = LogisticRegression(max_iter=100000, solver =best_solver, C=best_C).fit(X_train, y_train)\n",
    "\n",
    "# Get Accuracy\n",
    "preds = LR.predict(X_test)\n",
    "print('»» Best(?) Model Accuracy:' , accuracy_score(y_test, preds), '|Solver:', best_solver, '|C value:', best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f275b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is:  0.8784\n",
      "The Precision is:  0.8784\n",
      "The Recall is:  0.8784\n",
      "The F1 score is:  0.8784\n",
      "The Matthews correlation coefficient is:  0.8458\n",
      "\n",
      "This is the Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>648</td>\n",
       "      <td>13</td>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>304</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>23</td>\n",
       "      <td>364</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>412</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4\n",
       "0  648   13   55    4    6\n",
       "1    1  304   13    4   30\n",
       "2   51   23  364    6    0\n",
       "3   11    5    3  412   28\n",
       "4    9   27   11    4  468"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the final Logistic Regression Model\n",
    "LR = LogisticRegression(max_iter=100000, solver ='newton-cg', C=100).fit(X_train, y_train)\n",
    "\n",
    "# Show all the Model Evaluation Statistics for the best Hyperparameters found  \n",
    "preds = LR.predict(X_test)\n",
    "printClassResults(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7877333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bias is:  1.5024085649957584\n",
      "The other parameters are: \n",
      "\t B01 ->     2.588\n",
      "\t B02 ->     0.246\n",
      "\t B03 ->    -0.997\n",
      "\t B04 ->     0.371\n",
      "\t B05 ->     0.060\n",
      "\t B06 ->    -1.029\n",
      "\t B07 ->     0.064\n",
      "\t B08 ->     0.062\n",
      "\t B09 ->    -0.988\n",
      "\t B10 ->     0.013\n",
      "\t B11 ->    -0.228\n",
      "\t B12 ->    -1.043\n",
      "\t B13 ->    -0.318\n",
      "\t B14 ->    -0.208\n",
      "\t B15 ->    -0.860\n",
      "\t B16 ->    -0.362\n",
      "\t B17 ->     0.109\n",
      "\t B18 ->    -1.177\n",
      "\t B19 ->    -0.159\n",
      "\t B20 ->     0.028\n",
      "\t B21 ->    -0.733\n",
      "\t B22 ->    -0.244\n",
      "\t B23 ->     0.007\n",
      "\t B24 ->    -0.771\n",
      "\t B25 ->    -0.096\n",
      "\t B26 ->     0.012\n",
      "\t B27 ->     0.463\n",
      "\t B28 ->    -0.070\n",
      "\t B29 ->     0.061\n",
      "\t B30 ->     0.202\n",
      "\t B31 ->    -0.149\n"
     ]
    }
   ],
   "source": [
    "# Present the Bias and the Coefficients of the Model \n",
    "print(\"The bias is: \",  LR.intercept_[0])\n",
    "print(\"The other parameters are: \")\n",
    "for i, beta in enumerate(LR.coef_[0]):\n",
    "    print(\"\\t B%02d -> %9.3f\"% (i+1, beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6a5300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t B01 ->     2.588\n",
      "\t B18 ->     1.177\n",
      "\t B12 ->     1.043\n",
      "\t B06 ->     1.029\n",
      "\t B03 ->     0.997\n"
     ]
    }
   ],
   "source": [
    "# Present the Coefficients with the greatest impact\n",
    "coefs=[(abs(beta),i) for i, beta in enumerate(LR.coef_[0])]\n",
    "coefs.sort()\n",
    "coefs.reverse()\n",
    "for beta, i in coefs[:5]:\n",
    "    print(\"\\t B%02d -> %9.3f\"% (i+1, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2596e",
   "metadata": {},
   "source": [
    "## 3.2) Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2c810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver: auto | accuracy_score: 0.8316\n",
      "Solver: svd | accuracy_score: 0.8316\n",
      "Solver: cholesky | accuracy_score: 0.8316\n",
      "Solver: lsqr | accuracy_score: 0.8308\n",
      "Solver: sparse_cg | accuracy_score: 0.8316\n",
      "Solver: sag | accuracy_score: 0.8316\n",
      "Solver: saga | accuracy_score: 0.8316\n",
      "»Best Solver: auto : 0.8316\n",
      "###\n",
      "Alpha: 0.0001 | accuracy_score: 0.8316\n",
      "Alpha: 0.001 | accuracy_score: 0.8316\n",
      "Alpha: 0.01 | accuracy_score: 0.8316\n",
      "Alpha: 0.1 | accuracy_score: 0.8316\n",
      "Alpha: 1.0 | accuracy_score: 0.8316\n",
      "Alpha: 10 | accuracy_score: 0.8316\n",
      "Alpha: 100 | accuracy_score: 0.8316\n",
      "Alpha: 1000 | accuracy_score: 0.8364\n",
      "Alpha: 10000 | accuracy_score: 0.8264\n",
      "»Best Alpha: 1000 : 0.8364\n",
      "###\n",
      "»» Best(?) Model Accuracy: 0.8364 |Solver: auto |Alpha value: 1000\n"
     ]
    }
   ],
   "source": [
    "# Create various lists with different Hyperparameter values\n",
    "\n",
    "solvers = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 100, 1000, 10000]\n",
    "          \n",
    "best_solver = ''\n",
    "best_solver_value = 0\n",
    "          \n",
    "best_alpha = ''\n",
    "best_alpha_value = 0\n",
    "\n",
    "# Find the best Solver Hyperparameter\n",
    "for solver in solvers:\n",
    "    # Create a Ridge Classifier Model using a specific Solver\n",
    "    RC = RidgeClassifier(solver=solver).fit(X_train, y_train)\n",
    "\n",
    "    # Get Accuracy\n",
    "    preds = RC.predict(X_test)\n",
    "    a = accuracy_score(y_test, preds)\n",
    "    if a > best_solver_value:\n",
    "        best_solver_value = a\n",
    "        best_solver = solver\n",
    "    print(\"Solver:\", solver, \"| accuracy_score:\", a)\n",
    "\n",
    "print('»Best Solver:', best_solver, ':', best_solver_value)\n",
    "print('###')  \n",
    "\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Create a Ridge Classifier Model using a specific Alpha Value\n",
    "    RC = RidgeClassifier(alpha=alpha).fit(X_train, y_train)\n",
    "\n",
    "    # Get Accuracy\n",
    "    preds = RC.predict(X_test)\n",
    "    a = accuracy_score(y_test, preds)\n",
    "    if a > best_alpha_value:\n",
    "        best_alpha_value = a\n",
    "        best_alpha = alpha\n",
    "    print(\"Alpha:\", alpha, \"| accuracy_score:\", a)\n",
    "    \n",
    "print('»Best Alpha:', best_alpha, ':', best_alpha_value)\n",
    "print('###')\n",
    "\n",
    "# Create a Ridge Classifier Model with the best Hyperparameters found\n",
    "RC = RidgeClassifier(solver =best_solver, alpha=best_alpha).fit(X_train, y_train)\n",
    "\n",
    "# Get Accuracy\n",
    "preds = RC.predict(X_test)\n",
    "print('»» Best(?) Model Accuracy:' , accuracy_score(y_test, preds), '|Solver:', best_solver, '|Alpha value:', best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d74b28db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is:  0.8364\n",
      "The Precision is:  0.8364\n",
      "The Recall is:  0.8364\n",
      "The F1 score is:  0.8364\n",
      "The Matthews correlation coefficient is:  0.7932\n",
      "\n",
      "This is the Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>633</td>\n",
       "      <td>19</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>36</td>\n",
       "      <td>338</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>421</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "      <td>44</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4\n",
       "0  633   19   45   20    9\n",
       "1    0  298   20    2   32\n",
       "2   66   36  338    1    3\n",
       "3   13    8    1  421   16\n",
       "4    4   38   32   44  401"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the final Ridge Classifier Model\n",
    "RC = RidgeClassifier(solver=best_solver, alpha=best_alpha).fit(X_train, y_train)\n",
    "\n",
    "# Show all the Model Evaluation Statistics for the best Hyperparameters found  \n",
    "preds = RC.predict(X_test)\n",
    "printClassResults(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5687adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bias is:  -0.4405333333333332\n",
      "The other parameters are: \n",
      "\t B01 ->     0.304\n",
      "\t B02 ->     0.030\n",
      "\t B03 ->    -0.109\n",
      "\t B04 ->     0.024\n",
      "\t B05 ->     0.015\n",
      "\t B06 ->    -0.131\n",
      "\t B07 ->     0.001\n",
      "\t B08 ->     0.047\n",
      "\t B09 ->    -0.126\n",
      "\t B10 ->    -0.001\n",
      "\t B11 ->     0.015\n",
      "\t B12 ->    -0.133\n",
      "\t B13 ->    -0.018\n",
      "\t B14 ->    -0.009\n",
      "\t B15 ->    -0.102\n",
      "\t B16 ->    -0.028\n",
      "\t B17 ->     0.011\n",
      "\t B18 ->    -0.195\n",
      "\t B19 ->     0.013\n",
      "\t B20 ->     0.031\n",
      "\t B21 ->    -0.093\n",
      "\t B22 ->    -0.054\n",
      "\t B23 ->     0.067\n",
      "\t B24 ->    -0.130\n",
      "\t B25 ->     0.031\n",
      "\t B26 ->     0.006\n",
      "\t B27 ->     0.048\n",
      "\t B28 ->    -0.081\n",
      "\t B29 ->    -0.013\n",
      "\t B30 ->     0.029\n",
      "\t B31 ->    -0.093\n"
     ]
    }
   ],
   "source": [
    "# Present the Bias and the Coefficients of the Model\n",
    "print(\"The bias is: \",  RC.intercept_[0])\n",
    "print(\"The other parameters are: \")\n",
    "for i, beta in enumerate(RC.coef_[0]):\n",
    "    print(\"\\t B%02d -> %9.3f\"% (i+1, beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d90c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t B01 ->     0.304\n",
      "\t B18 ->     0.195\n",
      "\t B12 ->     0.133\n",
      "\t B06 ->     0.131\n",
      "\t B24 ->     0.130\n"
     ]
    }
   ],
   "source": [
    "# Present the Coefficients with the greatest impact \n",
    "coefs=[(abs(beta),i) for i, beta in enumerate(RC.coef_[0])]\n",
    "coefs.sort()\n",
    "coefs.reverse()\n",
    "for beta, i in coefs[:5]:\n",
    "    print(\"\\t B%02d -> %9.3f\"% (i+1, beta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
